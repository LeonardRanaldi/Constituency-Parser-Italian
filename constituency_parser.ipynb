{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk import Tree, grammar\n",
    "import random\n",
    "import queue\n",
    "import pickle as pkl\n",
    "from scipy.spatial import distance\n",
    "from PYEVALB import scorer\n",
    "from PYEVALB import parser\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "import math\n",
    "from itertools import product\n",
    "from time import time\n",
    "import argparse\n",
    "from tqdm import tqdm\n",
    "from sentence_splitter import SentenceSplitter, split_text_into_sentences\n",
    "\n",
    "\n",
    "\n",
    "def ignore_functional_labels(string):\n",
    "    \n",
    "    string = string.strip(' ')\n",
    "    l = string.split(' ')[1:]\n",
    "    for i in range(len(l)):\n",
    "        if l[i][0] == '(':\n",
    "            l[i] = l[i].split('~')[0]\n",
    "            \n",
    "    return ' '.join(l)[:-1]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_nodes(rule):\n",
    "    \n",
    "    if rule.is_lexical():\n",
    "        return True, set((rule._lhs._symbol,)), set((rule._rhs))\n",
    "    \n",
    "    else:\n",
    "        return False, set((rule._lhs._symbol,)).union(set((rule._rhs[0]._symbol, rule._rhs[1]._symbol))), set()\n",
    "    \n",
    "def grammar_infos(data):\n",
    "    \n",
    "    non_terminals = set()\n",
    "    pos_tags = set()\n",
    "    terminals = set()\n",
    "    binaries = set()\n",
    "    starts = set()\n",
    "    for string in data:\n",
    "        try:\n",
    "          t = Tree.fromstring(ignore_functional_labels(string))\n",
    "        except:\n",
    "          print(string)\n",
    "        t.chomsky_normal_form(horzMarkov=2)\n",
    "        t.collapse_unary(collapsePOS=True, collapseRoot=True)\n",
    "        rules = t.productions()\n",
    "        starts.add(rules[0]._lhs._symbol)\n",
    "        for rule in rules:\n",
    "            try:\n",
    "              lexical, non_terminal_nodes, terminal_nodes = extract_nodes(rule)\n",
    "            except:\n",
    "              pass\n",
    "            if lexical:\n",
    "                non_terminals = non_terminals.union(non_terminal_nodes)\n",
    "                pos_tags = pos_tags.union(non_terminal_nodes)\n",
    "                terminals = terminals.union(terminal_nodes)\n",
    "\n",
    "            else:\n",
    "                non_terminals = non_terminals.union(non_terminal_nodes)\n",
    "                if len(rule._rhs) == 2:\n",
    "                    binaries.add((rule._rhs[0]._symbol, rule._rhs[1]._symbol))\n",
    "                    \n",
    "    return non_terminals, pos_tags, terminals, binaries, starts\n",
    "\n",
    "def express_node(rule):\n",
    "\n",
    "    if rule.is_lexical():\n",
    "        return 'lexical', rule._lhs._symbol, rule._rhs[0].lower()\n",
    "    \n",
    "    else:\n",
    "        lhs, rhs = rule._lhs._symbol, (rule._rhs[0]._symbol, rule._rhs[1]._symbol)\n",
    "        if len(rhs) == 2:\n",
    "            return 'binary', lhs, rhs\n",
    "        \n",
    "def pcfg(data):\n",
    "    \n",
    "    # Initalize dictionnaries dict_lexicons, dict_unaries, dict_binaries that we put in dict_pcfg\n",
    "    dict_lexicons, dict_binaries = {}, {} # Notice that we already put the rule of the start node in\n",
    "                                                                              # Chomsky normal form\n",
    "    dict_pcfg = {'lexical' : dict_lexicons, 'binary' : dict_binaries}\n",
    "    \n",
    "    for string in data:\n",
    "        t = Tree.fromstring(ignore_functional_labels(string)) # Ignore the functional labels (see the doc of ignore_functional_labels)\n",
    "        t.chomsky_normal_form(horzMarkov=2)  # Convert the tree to Chomsky normal form\n",
    "        t.collapse_unary(collapsePOS=True, collapseRoot=True)\n",
    "        rules = t.productions()  # Get the rules\n",
    "        for rule in rules:       # We start by counting the rules in data\n",
    "            nature, lhs, rhs = express_node(rule)\n",
    "            if lhs in dict_pcfg[nature]:\n",
    "                if rhs in dict_pcfg[nature][lhs]:\n",
    "                    dict_pcfg[nature][lhs][rhs] += 1\n",
    "\n",
    "                else:\n",
    "                    dict_pcfg[nature][lhs][rhs] = 1\n",
    "\n",
    "            else:\n",
    "                dict_pcfg[nature][lhs] = {rhs : 1}\n",
    "                \n",
    "    dict_normalized = {}\n",
    "    for nature in dict_pcfg:\n",
    "        for lhs in dict_pcfg[nature]:\n",
    "            if lhs in dict_normalized:\n",
    "                dict_normalized[lhs] += sum(dict_pcfg[nature][lhs].values())\n",
    "                \n",
    "            else:\n",
    "                dict_normalized[lhs] = sum(dict_pcfg[nature][lhs].values())\n",
    "                \n",
    "    for nature in dict_pcfg:\n",
    "        for lhs in dict_pcfg[nature]:\n",
    "            dict_pcfg[nature][lhs] = dict((key, value/dict_normalized[lhs]) for key, value in dict_pcfg[nature][lhs].items())\n",
    "\n",
    "    dict_probas = {'lexical' : {}, 'unary' : {}, 'binary' : {}} # Initialize dict_probas\n",
    "    for nature in dict_pcfg:\n",
    "        for lhs in dict_pcfg[nature]:\n",
    "            for node in dict_pcfg[nature][lhs]:\n",
    "                if node in dict_probas[nature]:\n",
    "                    dict_probas[nature][node][lhs] = math.log(dict_pcfg[nature][lhs][node])\n",
    "\n",
    "                else:\n",
    "                    dict_probas[nature][node] = {lhs : math.log(dict_pcfg[nature][lhs][node])}\n",
    "                    \n",
    "    return dict_pcfg, dict_probas\n",
    "\n",
    "\n",
    "def cyk(sentence):\n",
    "    \n",
    "    n = len(sentence) + 1\n",
    "    k = len(non_terminals)\n",
    "    scores = [[{} for j in range(n)] for l in range(n)]\n",
    "    back = [[[None for i in range(k)] for j in range(n)] for l in range(n)]\n",
    "    Bs_scores, Cs_scores = [[set() for j in range(n)] for l in range(n)], [[set() for j in range(n)] for l in range(n)]\n",
    "    for i in range(0, n-1):\n",
    "        word = sentence[i]\n",
    "        if word in dict_probas['lexical'].keys():\n",
    "            for A in dict_probas['lexical'][word]:\n",
    "                scores[i][i+1][A] = dict_probas['lexical'][word][A]\n",
    "                if A in B_binary:\n",
    "                    Bs_scores[i][i+1].add(A)\n",
    "\n",
    "                if A in C_binary:\n",
    "                    Cs_scores[i][i+1].add(A)\n",
    "                    \n",
    "        else:\n",
    "            print(word + \" : isn't in terminals\")\n",
    "            return scores, back\n",
    "                 \n",
    "    for span in range(2, n):\n",
    "        start_time_binary = time()\n",
    "        for begin in range(0, n-span):\n",
    "            end = begin + span\n",
    "            start_time_binary = time()\n",
    "            for split in range(begin+1, end):\n",
    "                Bs = Bs_scores[begin][split].intersection(B_binary)\n",
    "                Cs = Cs_scores[split][end].intersection(C_binary)\n",
    "                \n",
    "                for B, C in set_binary.intersection(set(product(Bs, Cs))):\n",
    "                    score_B, score_C = scores[begin][split].get(B, -np.inf), scores[split][end].get(C, -np.inf)\n",
    "                    for A in dict_probas['binary'][(B, C)]:\n",
    "                        prob = score_B + score_C + dict_probas['binary'][(B, C)][A]\n",
    "                        if prob > scores[begin][end].get(A, -np.inf):\n",
    "                            scores[begin][end][A] = prob\n",
    "                            back[begin][end][dict_non_terminals_indices[A]] = (split, B, C)\n",
    "                            if A in B_binary:\n",
    "                                Bs_scores[begin][end].add(A)\n",
    "\n",
    "                            if A in C_binary:\n",
    "                                Cs_scores[begin][end].add(A)\n",
    "                                \n",
    "    return scores, back\n",
    "\n",
    "def not_in_grammar(sentence):\n",
    "\n",
    "    \n",
    "    string = '( (Not_in_Grammar '\n",
    "    for i in range(len(sentence)-1):\n",
    "        string += '(Not_in_Grammar ' + sentence[i] + ')'\n",
    "        \n",
    "    string += '(Not_in_Grammar ' + sentence[-1] + ')))'\n",
    "    return string\n",
    "\n",
    "def build_parentheses(back_track, scores, sentence):\n",
    "    \n",
    "    q = queue.LifoQueue()\n",
    "    begin = 0\n",
    "    end = len(back_track[0]) - 1\n",
    "    starts_scores = []\n",
    "    for start in starts:\n",
    "        starts_scores.append((scores[begin][end].get(start,-np.inf), start))\n",
    "    \n",
    "    starts_scores.sort(reverse=True)\n",
    "    if starts_scores[0][0] == -np.inf:\n",
    "        return not_in_grammar(sentence)\n",
    "    \n",
    "    sent = starts_scores[0][1]\n",
    "    string = '('\n",
    "    q.put((None, sent, 'start', 1, [begin, end]))\n",
    "\n",
    "    while not q.empty():\n",
    "        split, symbol, status, depth, border = q.get()   # border is begin for left and end for right\n",
    "        string += ' (' + symbol\n",
    "\n",
    "        if status == 'left':\n",
    "            border = [border[0], split]\n",
    "            children = back_track[border[0]][border[1]][dict_non_terminals_indices[symbol]]\n",
    "\n",
    "        elif status == 'right':\n",
    "            border = [split, border[1]]\n",
    "            children = back_track[border[0]][border[1]][dict_non_terminals_indices[symbol]]\n",
    "\n",
    "        elif status == 'start':\n",
    "            children = back_track[border[0]][border[1]][dict_non_terminals_indices[symbol]]\n",
    "\n",
    "        if children is not None:\n",
    "            if isinstance(children, grammar.Nonterminal):\n",
    "                node = (split, children, status, depth+1, border)\n",
    "                q.put(node)\n",
    "\n",
    "            elif len(children) == 3:\n",
    "                split, l_child, r_child = children\n",
    "                l_node, r_node = (split, l_child, 'left', 0, border), (split, r_child, 'right', depth+1, border)\n",
    "                q.put(r_node)\n",
    "                q.put(l_node)\n",
    "\n",
    "        else:\n",
    "            depth += 1\n",
    "            word = sentence[border[0]:border[1]][0]\n",
    "            string += ' ' + word + ')'*depth\n",
    "            \n",
    "    return string\n",
    "\n",
    "\n",
    "def levenshtein(s1, s2):\n",
    "    \n",
    "    \n",
    "    n1, n2 = len(s1), len(s2)\n",
    "    s1, s2 = s1.lower(), s2.lower()\n",
    "    lev = np.zeros((n1 + 1, n2 + 1))\n",
    "    for i in range(n1 + 1):\n",
    "        lev[i, 0] = i\n",
    "        \n",
    "    for j in range(n2 + 1):\n",
    "        lev[0, j] = j\n",
    "        \n",
    "    for i in range(1, n1 + 1):\n",
    "        for j in range(1, n2 + 1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                lev[i, j] = min(lev[i-1, j] + 1, lev[i, j-1] + 1, lev[i-1, j-1])\n",
    "                \n",
    "            else:\n",
    "                lev[i, j] = min(lev[i-1, j] + 1, lev[i, j-1] + 1, lev[i-1, j-1] + 1)\n",
    "                \n",
    "    return lev[n1, n2]\n",
    "\n",
    "\n",
    "def unigram(data):\n",
    "\n",
    "    probas = np.zeros(len(terminals))\n",
    "    for bracketed in data:\n",
    "        t = Tree.fromstring(bracketed)\n",
    "        for word in t.leaves():\n",
    "            probas[dict_terminals_indices[word]] += 1\n",
    "\n",
    "    return probas/probas.sum()\n",
    "\n",
    "def bigram(data):\n",
    "  \n",
    "    probas = np.ones((len(terminals), len(terminals)))\n",
    "    for bracketed in data:\n",
    "        t = Tree.fromstring(bracketed)\n",
    "        sentence = t.leaves()\n",
    "        if len(sentence) >= 2:\n",
    "            for i in range(1, len(sentence)):\n",
    "                index_1 = dict_terminals_indices[sentence[i - 1]] # The previous word in the sequene.\n",
    "                index_2 = dict_terminals_indices[sentence[i]] # The current word in the sequene.\n",
    "                probas[index_1, index_2] += 1\n",
    "\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    return probas/((probas.sum(axis = 1).reshape(-1, 1)))\n",
    "\n",
    "def closest_formal(s, terminals, n_words = 5, swap = False):\n",
    "\n",
    "    if swap:\n",
    "        fun = lambda word : damerau_levenshtein(s, word)\n",
    "        \n",
    "    else:\n",
    "        fun = lambda word : levenshtein(s, word)\n",
    "        \n",
    "    return {word : score for score, word in sorted(zip(list(map(fun, terminals)), terminals))[:n_words]}\n",
    "\n",
    "\n",
    "def closest_embedding(s, terminals, n_words = 10):\n",
    "\n",
    "    \n",
    "    fun = lambda word : distance.cosine(dict_words_embeddings[s], dict_terminals_embeddings[word])\n",
    "    return {score_word[1] : score_word[0] for score_word in sorted(zip(map(fun, terminals_embedded), terminals_embedded))[:n_words]}\n",
    "\n",
    "\n",
    "def choose_words(sentence, terminals, probas_unigram, probas_bigram, swap = True, n_words = 10, n_words_formal = 5, lambd = 0.8):\n",
    "        \n",
    "    log_proba = 0\n",
    "    sent = sentence.copy()\n",
    "    for i in range(len(sent)):\n",
    "        s = sent[i]\n",
    "        if s in terminals:\n",
    "            log_proba += math.log(probas_unigram[dict_terminals_indices[s]]) if i==0 else math.log(probas_bigram[dict_terminals_indices[sent[i-1]], dict_terminals_indices[s]])\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            words_formal = closest_formal(s, terminals, n_words_formal, swap)\n",
    "            words_embedding = closest_embedding(s, terminals, n_words) if s in dict_words_embeddings else {}\n",
    "            words_proposed = set(words_formal.keys()) | set(words_embedding.keys()) # Set of the proposed words\n",
    "            if i == 0:\n",
    "                scores = []\n",
    "                for word in words_proposed:\n",
    "                    unigram = math.log(probas_unigram[dict_terminals_indices[word]])\n",
    "                    scores.append((unigram, word))\n",
    "                \n",
    "            else:\n",
    "                scores = []\n",
    "                for word in words_proposed:\n",
    "                    bigram = math.log(lambd*probas_bigram[dict_terminals_indices[sent[i-1]], dict_terminals_indices[word]] + (1-lambd)*probas_unigram[dict_terminals_indices[word]])\n",
    "                    score = log_proba + bigram\n",
    "                    scores.append((score, word))\n",
    "                \n",
    "            scores.sort(reverse = True)\n",
    "            sent[i] = scores[0][1]\n",
    "            log_proba += scores[0][0]\n",
    "            \n",
    "    sent = [word.lower() for word in sent]\n",
    "    return sent\n",
    "\n",
    "def parse(sentence, n_words_formal=2, n_words=20, swap=False, lambd=0.8):\n",
    "    sentence = sentence.split(' ')\n",
    "    sentence_oov = choose_words(sentence, terminals, probas_unigram, probas_bigram, n_words_formal=n_words_formal, \n",
    "                                n_words=n_words, swap=swap, lambd=lambd)\n",
    "    scores, back = cyk(sentence_oov)\n",
    "    bracketed = build_parentheses(back, scores, sentence)\n",
    "    t_test = Tree.fromstring(bracketed)\n",
    "    t_test.un_chomsky_normal_form()\n",
    "    return \" \".join(str(t_test).split())\n",
    "    \n",
    "    \n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating the PCFG : Finished\n",
      "Defining the language model : Finished\n"
     ]
    }
   ],
   "source": [
    "data_train = 'IT_Treebank.txt'\n",
    "train_size = 0.9\n",
    "n_words_formal = 2\n",
    "n_words = 20\n",
    "swap = True\n",
    "lambd = 0.8\n",
    "\n",
    "\n",
    "# Load the training data\n",
    "with open(data_train, 'r', encoding = 'utf-8') as file:\n",
    "    data = file.read().splitlines()\n",
    "    \n",
    "file.close()\n",
    "  \n",
    "n_lines = int(train_size*len(data))\n",
    "data_test = data[n_lines:]\n",
    "data = data[:n_lines]\n",
    "# Prepare evaluation data, only keep the space-tokenized sentence from the bracketed parse.\n",
    "data_test_sentences = []\n",
    "for bracketed in data_test:\n",
    "    t = Tree.fromstring(bracketed)\n",
    "    data_test_sentences.append(\" \".join(t.leaves()))\n",
    "        \n",
    "# Describe the grammar of the training data\n",
    "non_terminals, pos_tags, terminals, binaries, starts = grammar_infos(data)\n",
    "    \n",
    "dict_non_terminals_indices = {non_terminal : index for index, non_terminal in enumerate(non_terminals)}\n",
    "dict_indices_non_terminals = {index : non_terminal for non_terminal, index in dict_non_terminals_indices.items()}\n",
    "\n",
    "dict_pos_indices = {pos : index for index, pos in enumerate(pos_tags)}\n",
    "dict_indices_pos = {index : pos for pos, index in dict_pos_indices.items()}\n",
    "\n",
    "dict_terminals_indices = {terminal : index for index, terminal in enumerate(terminals)}\n",
    "dict_indices_terminals = {index : terminal for terminal, index in dict_terminals_indices.items()}\n",
    "\n",
    "dict_binaries_indices = {binary : index for index, binary in enumerate(binaries)}\n",
    "dict_indices_terminals = {index : binary for binary, index in dict_binaries_indices.items()}\n",
    "\n",
    "dict_starts_indices = {binary : index for index, binary in enumerate(starts)}\n",
    "dict_indices_starts = {index : binary for binary, index in dict_starts_indices.items()}\n",
    "    \n",
    "  # Create pcfg\n",
    "dict_pcfg, dict_probas = pcfg(data)\n",
    "print('Creating the PCFG : Finished')\n",
    "    \n",
    "# Sets of left hand nodes and right hand nodes in binary rules.\n",
    "B_binary, C_binary = set([binary[0] for binary in dict_probas['binary'].keys()]), set([binary[1] for binary in dict_probas['binary'].keys()])\n",
    "set_binary = set(dict_probas['binary'].keys())\n",
    "    \n",
    "# Load polyglot embeddings.\n",
    "words, embeddings = pkl.load(open('polyglot-it.pkl', 'rb'), encoding = 'latin')\n",
    "dict_words_embeddings = dict((word, embedding) for word, embedding in zip(words, embeddings)) # Dictionnary mapping polyglot words to embeddings\n",
    "words_polyglot = set(words) # Set of all the words in polyglot dataset.\n",
    "dict_terminals_embeddings = {terminal : embedding for (terminal, embedding) in dict_words_embeddings.items() if terminal in terminals} # dictionnary mapping terminals to embeddings\n",
    "terminals_embedded = set(dict_terminals_embeddings.keys()) # Set of all terminals having an embedding.\n",
    "del embeddings\n",
    "    \n",
    "# Define language model.\n",
    "probas_unigram, probas_bigram = unigram(data), bigram(data)\n",
    "print('Defining the language model : Finished')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = 'Oggi è una bella giornata!'\n",
    "list_of_sentences = [sentence]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 49.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "( (S (ADVP (ADVB Oggi)) (VP (VMA è) (NP (ART una) (ADJ bella) (NOU giornata))) (. !)))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "splitter = SentenceSplitter(language='it')\n",
    "\n",
    "\n",
    "for sentence in tqdm(list_of_sentences):\n",
    "    sentences = []\n",
    "    list_parse = []\n",
    "    sentence = sentence.replace('...','.')\n",
    "    sentence = sentence.replace('.', ' . ').replace('!',' ! ').replace('?',' ? ')\n",
    "    sentences = splitter.split(text=sentence)\n",
    "    if len(sentences) == 1:\n",
    "        try:\n",
    "            parses = parse(sentences[0], n_words_formal=n_words_formal, n_words=n_words, swap=swap, lambd=lambd)\n",
    "        except:\n",
    "            parses = \"(S)\"\n",
    "    else:\n",
    "        for sub_sent in sentences:\n",
    "            try:\n",
    "                list_parse.append(parse(sub_sent, n_words_formal=n_words_formal, n_words=n_words, swap=swap, lambd=lambd))\n",
    "            except:\n",
    "                list_parse.append(\"(S)\")\n",
    "        parses = \"\"\n",
    "        for el in list_parse:\n",
    "            parses = parses + el\n",
    "        parses = \"(S \" + parses + \")\"\n",
    "    \n",
    "\n",
    "    print(parses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
